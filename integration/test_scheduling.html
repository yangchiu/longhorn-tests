<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.0">
<title>tests.test_scheduling API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>tests.test_scheduling</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="tests.test_scheduling.get_host_replica"><code class="name flex">
<span>def <span class="ident">get_host_replica</span></span>(<span>volume, host_id)</span>
</code></dt>
<dd>
<div class="desc"><p>Get the replica of the volume that is running on the test host. Trigger a
failed assertion if it can't be found.
:param volume: The volume to get the replica from.
:param host_id: The ID of the test host.
:return: The replica hosted on the test host.</p></div>
</dd>
<dt id="tests.test_scheduling.prepare_for_affinity_tests"><code class="name flex">
<span>def <span class="ident">prepare_for_affinity_tests</span></span>(<span>client, volume_name, request)</span>
</code></dt>
<dd>
<div class="desc"><p>For 'test_global_disk_soft_anti_affinity' and
'test_volume_disk_soft_anti_affinity' use, they have identical
the same preparation steps as below:</p>
<p>Given
- One node has three disks
- The three disks have very different sizes
- Only two disks are available for scheduling
- No other node is available for scheduling</p></div>
</dd>
<dt id="tests.test_scheduling.replica_auto_balance_with_data_locality_test"><code class="name flex">
<span>def <span class="ident">replica_auto_balance_with_data_locality_test</span></span>(<span>client, volume_name)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="tests.test_scheduling.reset_settings"><code class="name flex">
<span>def <span class="ident">reset_settings</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="tests.test_scheduling.test_allow_empty_disk_selector_volume_setting"><code class="name flex">
<span>def <span class="ident">test_allow_empty_disk_selector_volume_setting</span></span>(<span>client, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Test the global setting allow-empty-disk-selector-volume</p>
<p>If true, a replica of the volume without disk selector
can be scheduled on disk with tags.</p>
<p>If false, a replica of the volume without disk selector
can not be scheduled on disk with tags.</p>
<p>Setup
- Prepare 3 nodes each with one disk
- Add <code>AVAIL</code> tag to every disk
- Set allow-empty-disk-selector-volume to <code>false</code></p>
<p>When
- Create a Volume with 3 replicas without tag</p>
<p>Then
- All replicas can not be scheduled to the disks on the nodes</p>
<p>When
- Remove <code>AVAIL</code> tag from one of the node
- Set allow-empty-disk-selector-volume to <code>true</code></p>
<p>Then
- Wait for a while for controller to resync the volume,
all replicas can be scheduled to the disks on the nodes</p></div>
</dd>
<dt id="tests.test_scheduling.test_allow_empty_node_selector_volume_setting"><code class="name flex">
<span>def <span class="ident">test_allow_empty_node_selector_volume_setting</span></span>(<span>client, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Test the global setting allow-empty-node-selector-volume</p>
<p>If true, a replica of the volume without node selector
can be scheduled on node with tags.</p>
<p>If false, a replica of the volume without node selector
can not be scheduled on node with tags.</p>
<p>Setup
- Prepare 3 nodes
- Add <code>AVAIL</code> tag to nodes
- Set allow-empty-node-selector-volume to <code>false</code></p>
<p>When
- Create a Volume with 3 replicas without tag</p>
<p>Then
- All replicas can not be scheduled to the nodes</p>
<p>When
- Remove <code>AVAIL</code> tag from one of the node
- Set allow-empty-node-selector-volume to <code>true</code></p>
<p>Then
- Wait for a while for controller to resync the volume,
all replicas can be scheduled to the nodes</p></div>
</dd>
<dt id="tests.test_scheduling.test_data_locality_basic"><code class="name flex">
<span>def <span class="ident">test_data_locality_basic</span></span>(<span>client, core_api, volume_name, pod, settings_reset)</span>
</code></dt>
<dd>
<div class="desc"><p>Test data locality basic feature</p>
<p>Context:</p>
<p>Data Locality feature allows users to have an option to keep a local
replica on the same node as the consuming pod.
Longhorn is currently supporting 2 modes:
- disabled: Longhorn does not try to keep a local replica
- best-effort: Longhorn try to keep a local replica</p>
<p>See manual tests at:
<a href="https://github.com/longhorn/longhorn/issues/1045#issuecomment-680706283">https://github.com/longhorn/longhorn/issues/1045#issuecomment-680706283</a></p>
<p>Steps:</p>
<p>Case 1: Test that Longhorn builds a local replica on the engine node</p>
<ol>
<li>Create a volume(1) with 1 replica and dataLocality set to disabled</li>
<li>Find node where the replica is located on.
Let's call the node is replica-node</li>
<li>Attach the volume to a node different than replica-node.
Let call the node is engine-node</li>
<li>Write 200MB data to volume(1)</li>
<li>Use a retry loop to verify that Longhorn does not create
a replica on the engine-node</li>
<li>Update dataLocality to best-effort for volume(1)</li>
<li>Use a retry loop to verify that Longhorn creates and rebuilds
a replica on the engine-node and remove the other replica</li>
<li>detach the volume(1) and attach it to a different node.
Let's call the new node is new-engine-node and the old
node is old-engine-node</li>
<li>Wait for volume(1) to finish attaching</li>
<li>Use a retry loop to verify that Longhorn creates and rebuilds
a replica on the new-engine-node and remove the replica on
old-engine-node</li>
</ol>
<p>Case 2: Test that Longhorn prioritizes deleting replicas on the same node</p>
<ol>
<li>Add the tag AVAIL to node-1 and node-2</li>
<li>Set node soft anti-affinity to <code>true</code>.</li>
<li>Create a volume(2) with 3 replicas and dataLocality set to best-effort</li>
<li>Use a retry loop to verify that all 3 replicas are on node-1 and
node-2, no replica is on node-3</li>
<li>Attach volume(2) to node-3</li>
<li>User a retry loop to verify that there is no replica on node-3 and
we can still read/write to volume(2)</li>
<li>Find the node which contains 2 replicas.
Let call the node is most-replica-node</li>
<li>Set the replica count to 2 for volume(2)</li>
<li>Verify that Longhorn remove one replica from most-replica-node</li>
</ol>
<p>Case 3: Test that the volume is not corrupted if there is an unexpected
detachment during building local replica</p>
<ol>
<li>Remove the tag AVAIL from node-1 and node-2
Set node soft anti-affinity to <code>false</code>.</li>
<li>Create a volume(3) with 1 replicas and dataLocality set to best-effort</li>
<li>Attach volume(3) to node-3.</li>
<li>Use a retry loop to verify that volume(3) has only 1 replica on node-3</li>
<li>Write 2GB data to volume(3)</li>
<li>Detach volume(3)</li>
<li>Attach volume(3) to node-1</li>
<li>Use a retry loop to:
Wait until volume(3) finishes attaching.
Wait until Longhorn start rebuilding a replica on node-1
Immediately detach volume(3)</li>
<li>Verify that the replica on node-1 is in ERR state.</li>
<li>Attach volume(3) to node-1</li>
<li>Wait until volume(3) finishes attaching.</li>
<li>Use a retry loop to verify the Longhorn cleanup the ERR replica,
rebuild a new replica on node-1, and remove the replica on node-3</li>
</ol>
<p>Case 4: Make sure failed to schedule local replica doesn't block the
the creation of other replicas.</p>
<ol>
<li>Disable scheduling for node-3</li>
<li>Create a vol with 1 replica, <code>dataLocality = best-effort</code>.
The replica is scheduled on a node (say node-1)</li>
<li>Attach vol to node-3. There is a fail-to-schedule
replica with Spec.HardNodeAffinity=node-3</li>
<li>Increase numberOfReplica to 3. Verify that the replica set contains:
one on node-1, one on node-2,
one failed replica
with Spec.HardNodeAffinity=node-3.</li>
<li>Decrease numberOfReplica to 2. Verify that the replica set contains:
one on node-1, one on node-2,
one failed replica
with Spec.HardNodeAffinity=node-3.</li>
<li>Decrease numberOfReplica to 1. Verify that the replica set contains:
one on node-1 or node-2,
one failed replica
with Spec.HardNodeAffinity=node-3.</li>
<li>Decrease numberOfReplica to 2. Verify that the replica set contains:
one on node-1, one on node-2, one failed replica
with Spec.HardNodeAffinity=node-3.</li>
<li>
<p>Turn off data locality by set <code>dataLocality=disabled</code> for the vol.
Verify that the replica set contains: one on node-1, one on node-2</p>
</li>
<li>
<p>clean up</p>
</li>
</ol></div>
</dd>
<dt id="tests.test_scheduling.test_data_locality_strict_local_node_affinity"><code class="name flex">
<span>def <span class="ident">test_data_locality_strict_local_node_affinity</span></span>(<span>client, core_api, apps_api, storage_class, statefulset, request)</span>
</code></dt>
<dd>
<div class="desc"><p>Scenario: data-locality (strict-local) should schedule Pod to the same node</p>
<p>Issue: <a href="https://github.com/longhorn/longhorn/issues/5448">https://github.com/longhorn/longhorn/issues/5448</a></p>
<p>Given a StorageClass (lh-test) has dataLocality (strict-local)
And the StorageClass (lh-test) has numberOfReplicas (1)
And the StorageClass (lh-test) exists</p>
<p>And a StatefulSet (test-1) has StorageClass (lh-test)
And the StatefulSet (test-1) created.
And the StatefulSet (test-1) all Pods are in state (healthy).
And the StatefulSet (test-1) is deleted.
And the StatefulSet (test-1) PVC exists.</p>
<p>And a StatefulSet (test-2) has StorageClass (lh-test)
And the StatefulSet (test-2) has replicas (2)
And the StatefulSet (test-2) created.
And the StatefulSet (test-2) all Pods is in state (healthy).</p>
<p>When the StatefulSet (test-1) created.
Then the StatefulSet (test-1) all Pods are in state (healthy).</p></div>
</dd>
<dt id="tests.test_scheduling.test_global_disk_soft_anti_affinity"><code class="name flex">
<span>def <span class="ident">test_global_disk_soft_anti_affinity</span></span>(<span>client, volume_name, request)</span>
</code></dt>
<dd>
<div class="desc"><ol>
<li>When Replica Disk Soft Anti-Affinity is false, it should be impossible
to schedule replicas to the same disk.</li>
<li>When Replica Disk Soft Anti-Affinity is true, it should be possible to
schedule replicas to the same disk.</li>
<li>Whether or not Replica Disk Soft Anti-Affinity is true or false, the
scheduler should prioritize scheduling replicas to different disks.</li>
</ol>
<p>Given
- One node has three disks
- The three disks have very different sizes
- Only two disks are available for scheduling
- No other node is available for scheduling</p>
<p>When
- Global Replica Node Level Soft Anti-Affinity is true
- Global Replica Zone Level Soft Anti-Affinity is true
- Global Replica Disk Level Soft Anti-Affinity is false
- Create a volume with three replicas and a size such that all replicas
could fit on the largest disk and still leave it with the most available
space
- Attach the volume to the schedulable node</p>
<p>Then
- Verify the volume is in a degraded state
- Verify only two of the three replicas are healthy
- Verify the remaining replica doesn't have a spec.nodeID</p>
<p>When
- Change the global Replica Disk Level Soft Anti-Affinity to true</p>
<p>Then
- Verify the volume is in a healthy state
- Verify all three replicas are healthy (two replicas have the same
spec.diskID)</p>
<p>When
- Enable scheduling on the third disk
- Delete one of the two replicas with the same spec.diskID</p>
<p>Then
- Verify the volume is in a healthy state
- Verify all three replicas are healthy
- Verify all three replicas have a different spec.diskID</p></div>
</dd>
<dt id="tests.test_scheduling.test_hard_anti_affinity_detach"><code class="name flex">
<span>def <span class="ident">test_hard_anti_affinity_detach</span></span>(<span>client, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Test that volumes with Hard Anti-Affinity are still able to detach and
reattach to a node properly, even in degraded state.</p>
<ol>
<li>Create a volume and attach to the current node</li>
<li>Generate and write <code>data</code> to the volume.</li>
<li>Set <code>soft anti-affinity</code> to false</li>
<li>Disable current node's scheduling.</li>
<li>Remove the replica on the current node<ol>
<li>Verify volume will be in degraded state.</li>
<li>Verify volume reports condition <code>scheduled == false</code></li>
</ol>
</li>
<li>Detach the volume.</li>
<li>Verify that volume only have 2 replicas<ol>
<li>Unhealthy replica will be removed upon detach.</li>
</ol>
</li>
<li>Attach the volume again.<ol>
<li>Verify volume will be in degraded state.</li>
<li>Verify volume reports condition <code>scheduled == false</code></li>
<li>Verify only two replicas of volume are healthy.</li>
</ol>
</li>
<li>Check volume <code>data</code></li>
</ol></div>
</dd>
<dt id="tests.test_scheduling.test_hard_anti_affinity_live_rebuild"><code class="name flex">
<span>def <span class="ident">test_hard_anti_affinity_live_rebuild</span></span>(<span>client, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Test that volumes with Hard Anti-Affinity can build new replicas live once
a valid node is available.</p>
<p>If no nodes without existing replicas are available, the volume should
remain in "Degraded" state. However, once one is available, the replica
should now be scheduled successfully, with the volume returning to
"Healthy" state.</p>
<ol>
<li>Create a volume and attach to the current node</li>
<li>Generate and write <code>data</code> to the volume.</li>
<li>Set <code>soft anti-affinity</code> to false</li>
<li>Disable current node's scheduling.</li>
<li>Remove the replica on the current node<ol>
<li>Verify volume will be in degraded state.</li>
<li>Verify volume reports condition <code>scheduled == false</code></li>
</ol>
</li>
<li>Enable the current node's scheduling</li>
<li>Wait for volume to start rebuilding and become healthy again</li>
<li>Check volume <code>data</code></li>
</ol></div>
</dd>
<dt id="tests.test_scheduling.test_hard_anti_affinity_offline_rebuild"><code class="name flex">
<span>def <span class="ident">test_hard_anti_affinity_offline_rebuild</span></span>(<span>client, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Test that volumes with Hard Anti-Affinity can build new replicas during
the attaching process once a valid node is available.</p>
<p>Once a new replica has been built as part of the attaching process, the
volume should be Healthy again.</p>
<ol>
<li>Create a volume and attach to the current node</li>
<li>Generate and write <code>data</code> to the volume.</li>
<li>Set <code>soft anti-affinity</code> to false</li>
<li>Disable current node's scheduling.</li>
<li>Remove the replica on the current node<ol>
<li>Verify volume will be in degraded state.</li>
<li>Verify volume reports condition <code>scheduled == false</code></li>
</ol>
</li>
<li>Detach the volume.</li>
<li>Enable current node's scheduling.</li>
<li>Attach the volume again.</li>
<li>Wait for volume to become healthy with 3 replicas</li>
<li>Check volume <code>data</code></li>
</ol></div>
</dd>
<dt id="tests.test_scheduling.test_hard_anti_affinity_scheduling"><code class="name flex">
<span>def <span class="ident">test_hard_anti_affinity_scheduling</span></span>(<span>client, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Test that volumes with Hard Anti-Affinity work as expected.</p>
<p>With Hard Anti-Affinity, scheduling on nodes with existing replicas should
be forbidden, resulting in "Degraded" state.</p>
<ol>
<li>Create a volume and attach to the current node</li>
<li>Generate and write <code>data</code> to the volume.</li>
<li>Set <code>soft anti-affinity</code> to false</li>
<li>Disable current node's scheduling.</li>
<li>Remove the replica on the current node<ol>
<li>Verify volume will be in degraded state.</li>
<li>Verify volume reports condition <code>scheduled == false</code></li>
<li>Verify only two replicas of volume are healthy.</li>
</ol>
</li>
<li>Check volume <code>data</code></li>
</ol></div>
</dd>
<dt id="tests.test_scheduling.test_replica_auto_balance_disabled_volume_spec_enabled"><code class="name flex">
<span>def <span class="ident">test_replica_auto_balance_disabled_volume_spec_enabled</span></span>(<span>client, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Scenario: replica should auto-balance individual volume when
global setting <code>replica-auto-balance</code> is <code>disabled</code> and
volume spec <code>replicaAutoBalance</code> is <code>least_effort</code>.</p>
<p>Given set <code>replica-soft-anti-affinity</code> to <code>true</code>.
And set <code>replica-auto-balance</code> to <code>disabled</code>.
And disable scheduling for node-2.
disable scheduling for node-3.
And create volume-1 with 3 replicas.
create volume-2 with 3 replicas.
And set volume-2 spec <code>replicaAutoBalance</code> to <code>least-effort</code>.
And attach volume-1 to self-node.
attach volume-2 to self-node.
And wait for volume-1 to be healthy.
wait for volume-2 to be healthy.
And count volume-1 replicas running on each node.
And 3 replicas running on node-1.
0 replicas running on node-2.
0 replicas running on node-3.
And count volume-2 replicas running on each node.
And 3 replicas running on node-1.
0 replicas running on node-2.
0 replicas running on node-3.
And write some data to volume-1.
write some data to volume-2.</p>
<p>When enable scheduling for node-2.
enable scheduling for node-3.</p>
<p>Then count volume-1 replicas running on each node.
And 3 replicas running on node-1.
0 replicas running on node-2.
0 replicas running on node-3.
And count volume-2 replicas running on each node.
And 1 replicas running on node-1.
1 replicas running on node-2.
1 replicas running on node-3.
And volume-1 data should be the same as written.
And volume-2 data should be the same as written.</p></div>
</dd>
<dt id="tests.test_scheduling.test_replica_auto_balance_disk_in_pressure"><code class="name flex">
<span>def <span class="ident">test_replica_auto_balance_disk_in_pressure</span></span>(<span>client, core_api, apps_api, volume_name, statefulset, storage_class)</span>
</code></dt>
<dd>
<div class="desc"><p>Scenario: Test replica auto balance disk in pressure</p>
<p>Description: This test simulates a scenario where a disk reaches a certain
pressure threshold (80%), triggering the replica auto balance
to rebuild the replicas to another disk with enough available
space. Replicas should not be rebuilted at the same time.</p>
<p>Issue: <a href="https://github.com/longhorn/longhorn/issues/4105">https://github.com/longhorn/longhorn/issues/4105</a></p>
<p>Given setting "replica-soft-anti-affinity" is "false"
And setting "replica-auto-balance-disk-pressure-percentage" is "80"
And new 1Gi disk 1 is created on self node
new 1Gi disk 2 is created on self node
And disk scheduling is disabled for disk 1 on self node
disk scheduling is disabled for default disk on self node
And node scheduling is disabled for all nodes except self node
And new storageclass is created with <code>numberOfReplicas: 1</code>
And statefulset 0 is created with 1 replicaset
statefulset 1 is created with 1 replicaset
statefulset 2 is created with 1 replicaset
And all statefulset volume replicas are scheduled on disk 1
And data is written to all statefulset volumes until disk 1 is pressured
And disk 1 pressure is exceeded threshold (80%)</p>
<p>When enable disk scheduling for disk 1 on self node
And update setting "replica-auto-balance" to "best-effort"</p>
<p>Then at least 1 replicas should be rebuilt on disk 2
And at least 1 replica should not be rebuilt on disk 2
And disk 1 should be below disk pressure threshold (80%)
And all statefulset volume data should be intact</p></div>
</dd>
<dt id="tests.test_scheduling.test_replica_auto_balance_node_best_effort"><code class="name flex">
<span>def <span class="ident">test_replica_auto_balance_node_best_effort</span></span>(<span>client, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Scenario: replica auto-balance nodes with <code>best_effort</code>.</p>
<p>Given set <code>replica-soft-anti-affinity</code> to <code>true</code>.
And set <code>replica-auto-balance</code> to <code>best_effort</code>.
And disable scheduling for node-2.
disable scheduling for node-3.
And create a volume with 6 replicas.
And attach the volume to self-node.
And wait for the volume to be healthy.
And write some data to the volume.
And count replicas running on each node.
And 6 replicas running on node-1.
0 replicas running on node-2.
0 replicas running on node-3.</p>
<p>When enable scheduling for node-2.
And count replicas running on each node.
Then 3 replicas running on node-1.
3 replicas running on node-2.
0 replicas running on node-3.
And loop 3 times with each wait 5 seconds and count replicas on each nodes.
To ensure no addition scheduling is happening.
3 replicas running on node-1.
3 replicas running on node-2.
0 replicas running on node-3.</p>
<p>When enable scheduling for node-3.
And count replicas running on each node.
Then 2 replicas running on node-1.
2 replicas running on node-2.
2 replicas running on node-3.
And loop 3 times with each wait 5 seconds and count replicas on each nodes.
To ensure no addition scheduling is happening.
2 replicas running on node-1.
2 replicas running on node-2.
2 replicas running on node-3.</p>
<p>When check the volume data.
And volume data should be the same as written.</p></div>
</dd>
<dt id="tests.test_scheduling.test_replica_auto_balance_node_least_effort"><code class="name flex">
<span>def <span class="ident">test_replica_auto_balance_node_least_effort</span></span>(<span>client, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Scenario: replica auto-balance nodes with <code>least_effort</code>.</p>
<p>Given set <code>replica-soft-anti-affinity</code> to <code>true</code>.
And set <code>replica-auto-balance</code> to <code>least_effort</code>.
And disable scheduling for node-2.
disable scheduling for node-3.
And create a volume with 6 replicas.
And attach the volume to self-node.
And wait for the volume to be healthy.
And write some data to the volume.
And count replicas running on each nodes.
And 6 replicas running on node-1.
0 replicas running on node-2.
0 replicas running on node-3.</p>
<p>When enable scheduling for node-2.
Then count replicas running on each nodes.
And node-1 replica count != node-2 replica count.
node-2 replica count != 0.
node-3 replica count == 0.
And loop 3 times with each wait 5 seconds and count replicas on each nodes.
To ensure no addition scheduling is happening.
The number of replicas running should be the same.</p>
<p>When enable scheduling for node-3.
And count replicas running on each nodes.
And node-1 replica count != node-3 replica count.
node-2 replica count != 0.
node-3 replica count != 0.
And loop 3 times with each wait 5 seconds and count replicas on each nodes.
To ensure no addition scheduling is happening.
The number of replicas running should be the same.</p>
<p>When check the volume data.
And volume data should be the same as written.</p></div>
</dd>
<dt id="tests.test_scheduling.test_replica_auto_balance_with_data_locality"><code class="name flex">
<span>def <span class="ident">test_replica_auto_balance_with_data_locality</span></span>(<span>client, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Scenario: replica auto-balance should not cause rebuild loop.
- replica auto-balance set to <code>best-effort</code>
- volume data locality set to <code>best-effort</code>
- volume has 1 replica</p>
<p>Issue: <a href="https://github.com/longhorn/longhorn/issues/4761">https://github.com/longhorn/longhorn/issues/4761</a></p>
<p>Given no existing volume in the cluster.
And set <code>replica-auto-balance</code> to <code>best-effort</code>.
And create a volume:
- set data locality to <code>best-effort</code>
- 1 replica</p>
<p>When attach the volume to self-node.
And wait for the volume to be healthy.
Then the only volume replica should be already on the self-node or
get rebuilt one time onto the self-node.
And volume have 1 replica only and it should be on the self-node.
- check 15 times with 1 second wait interval</p>
<p>When repeat the test for 10 times.
Then should pass.</p></div>
</dd>
<dt id="tests.test_scheduling.test_replica_rebuild_per_volume_limit"><code class="name flex">
<span>def <span class="ident">test_replica_rebuild_per_volume_limit</span></span>(<span>client, core_api, storage_class, sts_name, statefulset)</span>
</code></dt>
<dd>
<div class="desc"><p>Test the volume always only have one replica scheduled for rebuild</p>
<ol>
<li>Set soft anti-affinity to <code>true</code>.</li>
<li>Create a volume with 1 replica.</li>
<li>Attach the volume and write a few hundreds MB data to it.</li>
<li>Scale the volume replica to 5.</li>
<li>Constantly checking the volume replica list to make sure there should be
only 1 replica in WO state.</li>
<li>Wait for the volume to complete rebuilding. Then remove 4 of the 5
replicas.</li>
<li>Monitoring the volume replica list again.</li>
<li>Once the rebuild was completed again, verify the data checksum.</li>
</ol></div>
</dd>
<dt id="tests.test_scheduling.test_replica_schedule_to_disk_with_most_usable_storage"><code class="name flex">
<span>def <span class="ident">test_replica_schedule_to_disk_with_most_usable_storage</span></span>(<span>client, volume_name, request)</span>
</code></dt>
<dd>
<div class="desc"><p>Scenario : test replica schedule to disk with the most usable storage</p>
<p>Given default disk 3/4 storage is reserved on the current node.
And disk-1 with 1/4 of default disk space + 10 Gi.
And add disk-1 to the current node.</p>
<p>When create and attach volume.</p>
<p>Then volume replica
on the current node scheduled to disk-1.
volume replica not on the current node scheduled to default disk.</p></div>
</dd>
<dt id="tests.test_scheduling.test_soft_anti_affinity_detach"><code class="name flex">
<span>def <span class="ident">test_soft_anti_affinity_detach</span></span>(<span>client, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Test that volumes with Soft Anti-Affinity can detach and reattach to a
node properly.</p>
<ol>
<li>Create a volume and attach to the current node.</li>
<li>Generate and write <code>data</code> to the volume</li>
<li>Set <code>soft anti-affinity</code> to true</li>
<li>Disable current node's scheduling.</li>
<li>Remove the replica on the current node</li>
<li>Wait for the new replica to be rebuilt</li>
<li>Detach the volume.</li>
<li>Verify there are 3 replicas</li>
<li>Attach the volume again. Verify there are still 3 replicas</li>
<li>Verify the <code>data</code>.</li>
</ol></div>
</dd>
<dt id="tests.test_scheduling.test_soft_anti_affinity_scheduling"><code class="name flex">
<span>def <span class="ident">test_soft_anti_affinity_scheduling</span></span>(<span>client, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Test that volumes with Soft Anti-Affinity work as expected.</p>
<p>With Soft Anti-Affinity, a new replica should still be scheduled on a node
with an existing replica, which will result in "Healthy" state but limited
redundancy.</p>
<ol>
<li>Create a volume and attach to the current node</li>
<li>Generate and write <code>data</code> to the volume.</li>
<li>Set <code>soft anti-affinity</code> to true</li>
<li>Disable current node's scheduling.</li>
<li>Remove the replica on the current node</li>
<li>Wait for the volume to complete rebuild. Volume should have 3 replicas.</li>
<li>Verify <code>data</code></li>
</ol></div>
</dd>
<dt id="tests.test_scheduling.test_soft_anti_affinity_scheduling_volume_disable"><code class="name flex">
<span>def <span class="ident">test_soft_anti_affinity_scheduling_volume_disable</span></span>(<span>client, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Test the global setting will be overwrite
if the volume disable the Soft Anti-Affinity</p>
<p>With Soft Anti-Affinity disabled,
scheduling on nodes with existing replicas should be forbidden,
resulting in "Degraded" state.</p>
<p>Setup
- Enable Soft Anti-Affinity in global setting</p>
<p>Given
- Create a volume with replicaSoftAntiAffinity=disabled in the spec
- Attach to the current node and Generate and write <code>data</code> to the volume</p>
<p>When
- Disable current node's scheduling.
- Remove the replica on the current node</p>
<p>Then
- Verify volume will be in degraded state.
- Verify volume reports condition <code>scheduled == false</code>
- Verify only two of volume are healthy.
- Check volume <code>data</code></p></div>
</dd>
<dt id="tests.test_scheduling.test_soft_anti_affinity_scheduling_volume_enable"><code class="name flex">
<span>def <span class="ident">test_soft_anti_affinity_scheduling_volume_enable</span></span>(<span>client, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Test the global setting will be overwrite
if the volume enable the Soft Anti-Affinity</p>
<p>With Soft Anti-Affinity, a new replica should still be scheduled on a node
with an existing replica, which will result in "Healthy" state but limited
redundancy.</p>
<p>Setup
- Disable Soft Anti-Affinity in global setting</p>
<p>Given
- Create a volume with replicaSoftAntiAffinity=enabled in the spec
- Attach to the current node and Generate and write <code>data</code> to the volume</p>
<p>When
- Disable current node's scheduling.
- Remove the replica on the current node</p>
<p>Then
- Wait for the volume to complete rebuild. Volume should have 3 replicas.
- Verify <code>data</code></p></div>
</dd>
<dt id="tests.test_scheduling.test_volume_disk_soft_anti_affinity"><code class="name flex">
<span>def <span class="ident">test_volume_disk_soft_anti_affinity</span></span>(<span>client, volume_name, request)</span>
</code></dt>
<dd>
<div class="desc"><ol>
<li>When Replica Disk Soft Anti-Affinity is disabled, it should be
impossible to schedule replicas to the same disk.</li>
<li>When Replica Disk Soft Anti-Affinity is enabled, it should be possible
to schedule replicas to the same disk.</li>
<li>Whether or not Replica Disk Soft Anti-Affinity is enabled or disabled,
the scheduler should prioritize scheduling replicas to different disks.</li>
</ol>
<p>Given
- One node has three disks
- The three disks have very different sizes
- Only two disks are available for scheduling
- No other node is available for scheduling</p>
<p>When
- Global Replica Node Level Soft Anti-Affinity is true
- Global Replica Zone Level Soft Anti-Affinity is true
- Create a volume with three replicas, a size such that all replicas could
fit on the largest disk and still leave it with the most available space,
and spec.replicaDiskSoftAntiAffinity = disabled
- Attach the volume to the schedulable node</p>
<p>Then
- Verify the volume is in a degraded state
- Verify only two of the three replicas are healthy
- Verify the remaining replica doesn't have a spec.nodeID</p>
<p>When
- Change the volume's spec.replicaDiskSoftAntiAffinity to enabled</p>
<p>Then
- Verify the volume is in a healthy state
- Verify all three replicas are healthy (two replicas have the same
spec.diskID)</p>
<p>When
- Enable scheduling on the third disk
- Delete one of the two replicas with the same spec.diskID</p>
<p>Then
- Verify the volume is in a healthy state
- Verify all three replicas are healthy
- Verify all three replicas have a different diskID</p></div>
</dd>
<dt id="tests.test_scheduling.wait_new_replica_ready"><code class="name flex">
<span>def <span class="ident">wait_new_replica_ready</span></span>(<span>client, volume_name, replica_names)</span>
</code></dt>
<dd>
<div class="desc"><p>Wait for a new replica to be found on the specified volume. Trigger a
failed assertion if one can't be found.
:param client: The Longhorn client to use in the request.
:param volume_name: The name of the volume.
:param replica_names: The list of names of the volume's old replicas.</p></div>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="tests" href="index.html">tests</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="tests.test_scheduling.get_host_replica" href="#tests.test_scheduling.get_host_replica">get_host_replica</a></code></li>
<li><code><a title="tests.test_scheduling.prepare_for_affinity_tests" href="#tests.test_scheduling.prepare_for_affinity_tests">prepare_for_affinity_tests</a></code></li>
<li><code><a title="tests.test_scheduling.replica_auto_balance_with_data_locality_test" href="#tests.test_scheduling.replica_auto_balance_with_data_locality_test">replica_auto_balance_with_data_locality_test</a></code></li>
<li><code><a title="tests.test_scheduling.reset_settings" href="#tests.test_scheduling.reset_settings">reset_settings</a></code></li>
<li><code><a title="tests.test_scheduling.test_allow_empty_disk_selector_volume_setting" href="#tests.test_scheduling.test_allow_empty_disk_selector_volume_setting">test_allow_empty_disk_selector_volume_setting</a></code></li>
<li><code><a title="tests.test_scheduling.test_allow_empty_node_selector_volume_setting" href="#tests.test_scheduling.test_allow_empty_node_selector_volume_setting">test_allow_empty_node_selector_volume_setting</a></code></li>
<li><code><a title="tests.test_scheduling.test_data_locality_basic" href="#tests.test_scheduling.test_data_locality_basic">test_data_locality_basic</a></code></li>
<li><code><a title="tests.test_scheduling.test_data_locality_strict_local_node_affinity" href="#tests.test_scheduling.test_data_locality_strict_local_node_affinity">test_data_locality_strict_local_node_affinity</a></code></li>
<li><code><a title="tests.test_scheduling.test_global_disk_soft_anti_affinity" href="#tests.test_scheduling.test_global_disk_soft_anti_affinity">test_global_disk_soft_anti_affinity</a></code></li>
<li><code><a title="tests.test_scheduling.test_hard_anti_affinity_detach" href="#tests.test_scheduling.test_hard_anti_affinity_detach">test_hard_anti_affinity_detach</a></code></li>
<li><code><a title="tests.test_scheduling.test_hard_anti_affinity_live_rebuild" href="#tests.test_scheduling.test_hard_anti_affinity_live_rebuild">test_hard_anti_affinity_live_rebuild</a></code></li>
<li><code><a title="tests.test_scheduling.test_hard_anti_affinity_offline_rebuild" href="#tests.test_scheduling.test_hard_anti_affinity_offline_rebuild">test_hard_anti_affinity_offline_rebuild</a></code></li>
<li><code><a title="tests.test_scheduling.test_hard_anti_affinity_scheduling" href="#tests.test_scheduling.test_hard_anti_affinity_scheduling">test_hard_anti_affinity_scheduling</a></code></li>
<li><code><a title="tests.test_scheduling.test_replica_auto_balance_disabled_volume_spec_enabled" href="#tests.test_scheduling.test_replica_auto_balance_disabled_volume_spec_enabled">test_replica_auto_balance_disabled_volume_spec_enabled</a></code></li>
<li><code><a title="tests.test_scheduling.test_replica_auto_balance_disk_in_pressure" href="#tests.test_scheduling.test_replica_auto_balance_disk_in_pressure">test_replica_auto_balance_disk_in_pressure</a></code></li>
<li><code><a title="tests.test_scheduling.test_replica_auto_balance_node_best_effort" href="#tests.test_scheduling.test_replica_auto_balance_node_best_effort">test_replica_auto_balance_node_best_effort</a></code></li>
<li><code><a title="tests.test_scheduling.test_replica_auto_balance_node_least_effort" href="#tests.test_scheduling.test_replica_auto_balance_node_least_effort">test_replica_auto_balance_node_least_effort</a></code></li>
<li><code><a title="tests.test_scheduling.test_replica_auto_balance_with_data_locality" href="#tests.test_scheduling.test_replica_auto_balance_with_data_locality">test_replica_auto_balance_with_data_locality</a></code></li>
<li><code><a title="tests.test_scheduling.test_replica_rebuild_per_volume_limit" href="#tests.test_scheduling.test_replica_rebuild_per_volume_limit">test_replica_rebuild_per_volume_limit</a></code></li>
<li><code><a title="tests.test_scheduling.test_replica_schedule_to_disk_with_most_usable_storage" href="#tests.test_scheduling.test_replica_schedule_to_disk_with_most_usable_storage">test_replica_schedule_to_disk_with_most_usable_storage</a></code></li>
<li><code><a title="tests.test_scheduling.test_soft_anti_affinity_detach" href="#tests.test_scheduling.test_soft_anti_affinity_detach">test_soft_anti_affinity_detach</a></code></li>
<li><code><a title="tests.test_scheduling.test_soft_anti_affinity_scheduling" href="#tests.test_scheduling.test_soft_anti_affinity_scheduling">test_soft_anti_affinity_scheduling</a></code></li>
<li><code><a title="tests.test_scheduling.test_soft_anti_affinity_scheduling_volume_disable" href="#tests.test_scheduling.test_soft_anti_affinity_scheduling_volume_disable">test_soft_anti_affinity_scheduling_volume_disable</a></code></li>
<li><code><a title="tests.test_scheduling.test_soft_anti_affinity_scheduling_volume_enable" href="#tests.test_scheduling.test_soft_anti_affinity_scheduling_volume_enable">test_soft_anti_affinity_scheduling_volume_enable</a></code></li>
<li><code><a title="tests.test_scheduling.test_volume_disk_soft_anti_affinity" href="#tests.test_scheduling.test_volume_disk_soft_anti_affinity">test_volume_disk_soft_anti_affinity</a></code></li>
<li><code><a title="tests.test_scheduling.wait_new_replica_ready" href="#tests.test_scheduling.wait_new_replica_ready">wait_new_replica_ready</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.0</a>.</p>
</footer>
</body>
</html>
